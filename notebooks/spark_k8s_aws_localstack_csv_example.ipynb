{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "streaming-punishment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "swan_spark_conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local\")\n",
    "swan_spark_conf \\\n",
    "  .set(\"spark.kubernetes.namespace\", \"spark-ml\") \\\n",
    "  .set(\"spark.kubernetes.authenticate.executor.serviceAccountName\", \"spark-ml-executor\") \\\n",
    "  .set(\"spark.executor.instances\", \"2\") \\\n",
    "  .set(\"spark.executor.memory\", \"2g\") \\\n",
    "  .set(\"spark.executor.cores\", \"2\") \\\n",
    "  .set(\"spark.driver.memory\", \"1G\") \\\n",
    "  .set(\"spark.kubernetes.container.image\", \"spark:3.0.1_python386\") \\\n",
    "  .set(\"spark.driver.port\", \"2222\") \\\n",
    "  .set(\"spark.driver.blockManager.port\", \"7777\") \\\n",
    "  .set(\"spark.driver.host\", \"jupyter.recs.svc.cluster.local\") \\\n",
    "  .set(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "  .set(\"spark.ui.port\", \"4040\") \\\n",
    "  .set(\"spark.network.timeout\", \"240\") \\\n",
    "  .set(\"spark.driver.extraClassPath\", \"/opt/conda/lib/python3.8/site-packages/sparkmonitor/listener_2.12.jar:/opt/conda/lib/python3.8/site-packages/pyspark/jars/hadoop-aws-3.2.0.jar:/opt/conda/lib/python3.8/site-packages/pyspark/jars/aws-java-sdk-bundle-1.11.888.jar\") \\\n",
    "  .set(\"spark.executor.extraClassPath\", \"/opt/conda/lib/python3.8/site-packages/pyspark/jars/hadoop-aws-3.2.0.jar:/opt/conda/lib/python3.8/site-packages/pyspark/jars/aws-java-sdk-bundle-1.11.888.jar\") \\\n",
    "  .set(\"spark.hadoop.fs.s3a.endpoint\", \"localstack.kube-system.svc.cluster.local:4566\") \\\n",
    "  .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "  .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "  .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "  .set(\"spark.hadoop.com.amazonaws.services.s3.enableV4\", \"true\") \\\n",
    "  .set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "\n",
    "# print(swan_spark_conf.toDebugString()) #Instance of SparkConf with options set by the extension\n",
    "\n",
    "spark = SparkSession.builder.appName(\"test\").config(conf=swan_spark_conf).getOrCreate()\n",
    "\n",
    "try:\n",
    "    df = spark.read.csv('s3a://my-bucket/stocks.csv',header=True)\n",
    "    df.printSchema()\n",
    "    print(df.count())\n",
    "except Exception as exp:\n",
    "    print(exp)\n",
    "    \n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-language",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-webmaster",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
